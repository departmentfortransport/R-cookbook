--- 
title: "DfT R Cookbook"
author: "Isi Avbulimen, Hannah Bougdah, Tamsin Forbes, Andrew Kelly, Tim Taylor"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: 
  bookdown::gitbook:
    df_print: kable
    fig_width: 7
    fig_height: 6
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: departmentfortransport/R-cookbook
description: "Guidance and code examples for R usage for DfT and beyond"

---
```{r include=FALSE}
require(dplyr)
require(knitr)

options(knitr.table.format = "html")

# Load example data for Road Casualties in Great Britain 1969-84
data(mtcars)

# Look at the data
mtcars <- tibble::as.tibble(mtcars) %>%
  # Add rows for dplyr example
  tibble::rownames_to_column(var = "car")
```

# Why do we need _another_ book?

`R` is a very flexible programming language, which inevitably means there are lots of ways to achieve the same result. This is true of all programming languages, but is particularly exaggerated in `R` which makes use of ['meta-programming'](http://adv-r.had.co.nz/). 

For example, here is how to calculate a new variable using standard R and filter on a variable:
```{r}
# Calculate kilometers per litre from miles per gallon
mtcars$kpl <- mtcars$mpg * 0.425144

# Select cars with a horsepower greater than 250 & show only mpg and kpl columns
mtcars[mtcars$hp > 250, c("car", "mpg", "kpl")]

```

Here's the same thing using Tidyverse style R:
```{r}

mtcars %>%
  # Calculate kilometers per litre
  dplyr::mutate(
    kpl = mpg * 0.425144
  ) %>%
  # Filter cars with a horsepower greater than 250
  dplyr::filter(
    hp > 250
  ) %>%
  # Take only the car, mpg, and newly created kpl columns
  dplyr::select(car, mpg, kpl)
```

These coding styles are quite different. As people write more code across the Department, it will become increasingly important that code can be handed over to other R users. It is much easier to pick up code written by others if it uses the same coding style you are familiar with. 

This is the main motivation for this book, to establish a way of coding that represents a sensible default for those who are new to R that is readily transferable across DfT. 


## Coding standards

Related to this, the Data Science team maintain a [coding standards document](https://departmentfortransport.github.io/ds-processes/Coding_standards/r.html), that outlines some best practices when writing R code. This is not prescriptive and goes beyond the scope of this document, but might be useful for managing your R projects. 

## Adding to the book

This book is not static - new chapters can be added and current chapters can be amended. If you want to edit or write a chapter for the book, check out the [GitHub page](https://github.com/departmentfortransport/ds-r-training). 


```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), "bookdown", "knitr", "rmarkdown"
), "packages.bib")
```


<!--chapter:end:index.Rmd-->

# The basics {#basics}

```{r include=FALSE}
# Every file must contain at least one R chunk due to the linting process.
```

**TO BE COMPLETED**

## Vectors

### Types {#vector-types}

There are four main atomic vector types that you are likely to come across
when using R^[technically there are more, see 
https://adv-r.hadley.nz/vectors-chap.html#atomic-vectors]; **logical** (`TRUE` 
or `FALSE`), *double* (`3.142`), *integer* (`2L`) and *character* (`"Awesome"`)

```{r}
v1 <- TRUE
typeof(v1)
v1 <- FALSE
typeof(v1)

v2 <- 1.5
typeof(v2)
v2 <- 1
typeof(v2)

# integer values must be followed by an L to be stored as integers
v3 <- 2
typeof(v3)
v3 <- 2L
typeof(v3)

v4 <- "Awesome"
typeof(v4)
```

As well as the atomic vector types you will often encounter two other vector
types; **factor** and **Date**.  

Factor vectors are used to represent categorical data.  They are actually
integer vectors with two additional attributes, levels and class.  At this
stage it is not worth worrying to much about what attribute are just understand
that, for factors, the levels attribute gives the possible categories and 
combined with the integer values works much like a lookup table.  The `class` 
attribute is just "factor".


```{r}
ratings <- factor(c("good", "bad", "bad", "amazing"))
typeof(ratings)
attributes(ratings)
```

Date vectors are just double vectors with an additional class attribute set as
"Date".  

**Note - The following borrows directly from Hadley's Advanced R site
so we need to attribute appropriately and use correct licence**

```{r}
today <- Sys.Date()
typeof(today)
attributes(today)
```

The value of the double (which can be seen by stripping the class), represents
the number of days since "1970-01-01"^[This special date is known as the Unix 
Epoch]:

```{r}
date <- as.Date("1970-02-01")
unclass(date)
```

### Conversion between atomic vector types

Converting between the atomic vector types is done using the `as.character`, 
`as.integer`, `as.logical` and `as.double` functions.

```{r}
value <- 1.5
as.integer(value)
as.character(value)
as.logical(value)
```

Where it is not possible to convert a value you will get a warning message

```{r}
value <- "z"
as.integer(value)
```

When combining different vector types, coercion will obey the following 
hierarchy: character, double, integer, logical.

```{r}
typeof(c(9.9, 3L, "pop", TRUE))
typeof(c(9.9, 3L, TRUE))
typeof(c(3L, TRUE))
typeof(TRUE)
```

<!--chapter:end:01-basics.Rmd-->

# Data Importing/Exporting and interaction with other programmes {#data-import}

This chapter is for code examples of data importing/exporting and interactions with other programmes and databases.

## Libraries

```{r message=FALSE}
library(tidyverse)
library(DBI)
library(dbplyr)
library(haven)
library(bigrquery)
library(fs)
library(haven)
```

## Navigating folders

A couple of pointers to navigate from your working directory, which, if you're using R projects (it is highly recommended that you do) will be wherever the `.Rproj` file is located 

### Down

To navigate down folders use `/`. The path given below saves the file **my_df.csv** in the **data** folder, which itself is inside the **monthly_work** folder
```{r eval=FALSE}
readr::write_csv(
  x = my_dataframe
  , path = "monthly_work/data/my_df.csv"
)
```

### Up

To go up a folder use `../`. In particular you may need to do this when running Rmarkdown files. Rmarkdown files use their location as the working directory. If you have created an **R** folder, say, to stash all your scripts in, and a **data** folder to stash your data files in, then you will need to go up, before going down...

The path below goes up one folder, then into the **data** folder, where the **lookup_table.csv** is located.
```{r eval=FALSE}
lookup_table <- readr::read_csv(
  file = "../data/lookup_table.csv"
)
```

## Working with files in R

This section focusses on reading in various filetypes for working on in R memory. 

### .rds

.rds is R's native file format, any object you create in R can be saved as a .rds file. The functions `readRDS` and `saveRDS` are base R functions.

```{r eval=FALSE}
#not run
saveRDS(
  object = my_model #specify the R object you want to save
  , file = "2019_0418_my_model.rds" #give it a name, don't forget the file extension
)
```


### .csv

We use the functions `read_csv` and `write_csv` from the `readr` package (which is part of the `tidyverse`). These are a little bit *cleverer* than their base counterparts, however, this cleverness can catch you out.

#### `readr::read_csv`

The file **messy_pokemon_data.csv** contains pokemon go captures data which has been deliberately messed up a bit. `read_csv` imputes the column specification from the first 1000 rows, which is fine if your first 1000 rows are representative of the data type. If not then subsequent data that can't be coerced into the imputed data type will be replaced with NA. 

Looking at the column specification below notice that `read_csv` has recognised **time_first_capture** as a time type, but not **date_first_capture** as date type. Given the information that **combat_power** should be numeric we can see that something is also amiss here as `read_csv` has guessed character type for this column.  
```{r}
pokemon <- readr::read_csv(
  file = "data/messy_pokemon_data.csv"
)
```

Let's have a quick look at some data from these columns
```{r}
pokemon %>% 
  dplyr::select(species, combat_power, date_first_capture, time_first_capture) %>% 
  dplyr::arrange(desc(combat_power)) %>% 
  head()
```

The pokemon dataset has less than 1000 rows so `read_csv` has 'seen' the letters mixed in with some of the numbers in the **combat_power** column. It has guessed at character type because everything in the column can be coerced to character type.

What if there are more than 1000 rows? For example, say you have a numeric column, but there are some letters prefixed to the numbers in some of the post-row-1000 rows. These values are still meaningful to you, and with some data wrangling you can extract the actual numbers. Unfortunately `read_csv` has guessed at type double based on the first 1000 rows and since character type cannot be coerced into double, these values will be replaced with `NA`. If you have messy data like this the best thing to do is to force `read_csv` to read in as character type to preserve all values as they appear, you can then sort out the mess yourself.

You can specify the column data type using the `col_types` argument. Below I have used a compact string of abbreviations to specify the column types, see the help at `?read_csv` or the `readr` vignette for the full list. You can see I got many parsing failures, which I can access with `problems()`. This is a data frame of the values that `read_csv` was unable to coerce into the type I specified, and so has replaced with NA. 
```{r}
pokemon <- readr::read_csv(
  file = "data/messy_pokemon_data.csv"
  , col_types = "cdddcdcccDt"
)
# c = character, d = double, D = Date, t = time
```
Let's take a look at the problems.
```{r}
problems(pokemon) %>% 
  head()
```

And since I know that there are problems with **combat_power** let's take a look there.
```{r}
problems(pokemon) %>% 
  dplyr::filter(col == "combat_power") %>% 
  head()
  
```
The `problems()` feature in `read_csv` is super useful, it helps you isolate the problem data so you can fix it.

Other arguments within `read_csv` that I will just mention, with their default settings are

- `col_names = TRUE`: the first row on the imput is used as the column names.
- `na = c("", "NA")`: the default values to interpret as `NA`.
- `trim_ws = TRUE`: by default trims leading/trailing white space. 
- `skip = 0`: number of lines to skip before reading data.
- `guess_max = min(1000, n_max)`: maximum number of records to use for guessing column type. NB the bigger this is the longer it will take to read in the data.

#### `readr::write_csv`

There are many `write_` functions in the `readr` package. I rarely use anything other than `write_csv`, but some are faster or have other functionality. See the full list by running `?write_csv` in the console.

Set the path relative to your R project or working directory, and name the file including the file extension ".csv". Pay attention to the defaults, for instance you might want to change the default `na = "NA"` to `na = ""` so that __NA__'s in your data are written to blanks in your .csv rather than the character string "NA". This is important if "NA" means something else in your data, for example, if you are working with country codes including "NA" for Namibia...

```{r eval=FALSE}
#not run

readr::write_csv(
  x = pokemon
  , path = "data/pokemon.csv"
  , na = "NA" #default - take care if dealing with country code NA for Namibia!
  , append = FALSE #default is not to append, if the file exists it will be overwritten
  , col_names = !append #default is the opposite of append, since if appending you don't want the col_names repeated
  , quote_escape = "double" #default - eg use two types of quote if you want the output quoted; eg "'quoted output'" will be written as 'quoted output'.
)

```





### Importing .xlsx and .xls

Excel workbooks come in many shapes and sizes. You may have one or many worksheets in one or many workbooks, there may only be certain cells that you are interested in. Below are a few examples of how to cope with these variations using `readxl` and `purrr` to iterate over either worksheets and/or workbooks.

#### A single worksheet in a single workbook

The simplest combination, you are interested in one rectangular datset in a particular worksheet in one workbook. Leaving the defaults works fine on this dataset. Note that `readxl::read_excel` detects if the file is `.xlsx` or `.xls` and behaves accordingly.

```{r}
readxl::read_excel(path = "data/port0499.xlsx") %>% 
  head()
```

Let's set a few of the other arguments, run `?read_excel` in the console to see the full list.
```{r}
readxl::read_excel(
  path = "data/port0499.xlsx"
  , sheet = 1 #number or name of sheet, default is first sheet
  , col_names = TRUE #default
  , col_types = "text" #a single type will recycle to all columns, specify each using character vector of the same length eg c("numeric", "text", ...)
) %>% 
  head()
```


#### A single worksheet in many workbooks

For example, you collect pokemon go capture data from many different players, the data all has the same struture and you want to read it in and row bind into a single dataframe in R. 
The code below collects the names of the 3 excel workbooks using `fs::dir_ls`, and, as these are not the only files in that folder, I've specified them using regular expressions (regex). Then we use `purrr::map_df` to iterate over this list of files, applying the function we supply, that is `readxl::read_excel`. The `.id` argument adds the file path into a new column, which we have named "player" in this instance.

```{r}

pokemon <- fs::dir_ls(path = "data", regex = "pokemon_player_.\\.xlsx$")  %>% 
  purrr::map_df(readxl::read_excel, .id = "player")

DT::datatable(data = pokemon)

```

Note that the `regex` argument in `fs::dir_ls` is applied to the full file path so if I had tried to specify that the file name starts with "pokemon" by front anchoring it using "^pokemon" this would return no results, since the full name is actually "data/pokemon...". 
[regex cheatsheet](https://www.rstudio.com/wp-content/uploads/2016/09/RegExCheatsheet.pdf)
[stringr cheatsheet including regex](http://edrub.in/CheatSheets/cheatSheetStringr.pdf)

#### Many worksheets in a single workbook

You have a single workbook, but it contains many worksheets of interest, each containing rectangular data with the same structure. For example, you have a workbook containing pokemon go captures data, where each different data collection point has its own sheet. The data structure, column names and data types are consistent. You want to read in and combine these data into a single dataframe.

The code below sets the location of the workbook and puts this in the object `path`. It then collects the names of all the sheets in that workbook using `readxl::excel_sheets`. Next `purrr::set_names` sets these names in a vector so that they can be used in the next step. This vector of names is implictly assigned to the `.x` argument in `purrr::map_df` as it is the first thing passed to it. This means we can refer to it as `.x` in the function we are iterating, in this case `readxl::read_excel`. Finally, an id column is included, made up of the sheet names and named "sheet". The output is a single dataframe with all the sheets row bound together.

```{r}
path <- "data/multi_tab_messy_pokemon_data.xlsx"

pokemon_collections <- readxl::excel_sheets(path = path) %>% 
  purrr::set_names() %>% 
   purrr::map_df(
     ~ readxl::read_excel(path = path, sheet = .x)
     , .id = "sheet"
   )

DT::datatable(data = pokemon_collections)

```

#### Many worksheets in many workbooks

Now we can use the above two solutions to combine data from many worksheets spread across many workbooks. As before, the data is rectangular and has the same structure. For example, you receive a workbook every month, containing pokemon go captures data, and data collection point has its own sheet. 

We create a function to import and combine the sheets from a single workbook, and then iterate this function over all the workbooks using `purrr::map_df`.

```{r}
#function to combine sheets from a single workbook
read_and_combine_sheets <- function(path){
  readxl::excel_sheets(path = path) %>% 
  purrr::set_names() %>% 
   purrr::map_df(
     ~ readxl::read_excel(path = path, sheet = .x)
     , .id = "sheet"
   )
}

#code to iterate over many workbooks
pokemon_monthly_collections <- fs::dir_ls(
  path = "data", regex = "pokemon_2019\\d{2}\\.xlsx$")  %>% 
  purrr::map_df(
    read_and_combine_sheets
    , .id = "month"
    )

DT::datatable(data = pokemon_monthly_collections)
```

#### Non-rectangular data in many workbooks

You have received some kind of data entry form that has been done in excel in a more human readable, rather than machine readable, format. Some of the cells contain instructions and admin data so you only want the data held in specific cells. This is non-rectangular data, that is, the data of interest is dotted all over the place. In this example we have pet forms, one sheet per workbook, and the data of interest is in cells B2, D5 and E8 only.

Let's see what we get if we naively try to read in one of these forms.
```{r message=FALSE}
readxl::read_excel(
  path = "data/pet_form_1.xlsx"
) %>% 
  head()

```
It's not what we wanted, let's try again, now using the `range` argument
```{r message=FALSE}
readxl::read_excel(
  path = "data/pet_form_1.xlsx"
  , col_names = FALSE
  , range = "A2:B2"
)
```
The `range` argument helps, we have picked up one bit of the data, and it's name. The `range` argument uses the `cellranger` package which allows you to refer to ranges in Excel files in Excel style. However, we have 3 disconnected data points, we need to iterate, so it's `purrr::map_df` to the rescue once more.
```{r}
pet_details <- purrr::map_df(
    .x = c("B2", "D5", "E8")
    , ~ readxl::read_excel(
      path = "data/pet_form_1.xlsx"
      , range = .x
      , col_names = "cells"
      , col_types = "text" #have to use text to preserve all data in single column
    ) 
  ) 
pet_details
```
This is pretty close to what we want, we have a dataframe named `pet_details` comprising a single "cells" column, which contains all the relevant data from this worksheet. Now we can reshape it. Below I create a tibble with the column names I want in the first column, under "cell_name" and pull the data from `pet_details` into it, then spread it.
```{r}
pet <- tibble::tibble(
    cell_name = c("Name", "Age", "Species")
    , values = dplyr::pull(pet_details)
  ) %>% 
    dplyr::mutate(
      cell_name = as_factor(cell_name) #convert to factor to preserve order
      ) %>% 
    tidyr::spread(
      key = cell_name #make variable name the key and the values the value
      , value = values
      )
pet
```
Having solved for one workbook and sheet, we can functionalise and iterate to gather the data from every wookbook. (Note if you have many worksheets as well, you'll need another interim iteratative step)

The function `get_cells_and_reshape` below iterates over reading each of the three cells from the worksheet, combines these into a single column dataframe, then adds the cell names and reshapes the data into tidy format. It takes three inputs, `path`, `cells`, `cell_name`, all of which are type character or character vector.
```{r}
get_cells_and_reshape <- function(path, cells, cell_name){
  purrr::map_df(
    .x = cells
    , ~ readxl::read_excel(
      path = path
      , range = .x
      , col_names = "details"
      , col_types = "text" #have to use text to preserve all data in single column
    ) 
  ) %>% 
  dplyr::pull() %>% #pull just the values from the previous df
  tibble::tibble( #create df with 2 columns, cell_name and these values
    cell_name = c("Name", "Age", "Species")
    , values = . #use . to refer to what was piped, when not in first position
  ) %>% 
  dplyr::mutate(
    cell_name = as_factor(cell_name) #convert to factor to preserve order
  ) %>% 
  tidyr::spread( #this is 
    key = cell_name #make variable name the key and the values the value
    , value = values
  )  
}
```

Let's test it on the first pet form data, first setting the paramaters to use in the function. 
```{r}
path <- "data/pet_form_1.xlsx"
cells <- c("B2", "D5", "E8")
cell_name <- c("Name", "Age", "Species")

get_cells_and_reshape(
  path = path, cells = cells, cell_name = cell_name)
```

It works! So now we can iterate this over all the pet form workbooks, specifying the paths using regex as before. Note we use `.x` in the `path` argument in the `get_cells_and_reshape` function to refer to the vector of paths piped to `purrr::map_df` from `fs::dir_ls`. 
```{r}
fs::dir_ls(
  path = "data", regex = "pet_form_\\d\\.xlsx$")  %>% 
  purrr::map_df(
    ~ get_cells_and_reshape(path = .x, cells = cells, cell_name = cell_name)
    , .id = "path"
    )
```

### Exporting to .xlsx

We recommend `openxlsx` and `xltabr` for writing and formatting tables to MS Excel. The latter is a wrapper built on `openxlsx` developed by MoJ and is specifically aimed at making it easier to produce publication ready tables. `xltabr` has one known drawback in that it applies the foramtting cell by cell, so if your tables are massive (~100,000 rows) it will take too long, in this instance you should resort to `openxlsx`.

https://cran.r-project.org/web/packages/openxlsx/openxlsx.pdf

https://cran.r-project.org/web/packages/xltabr/xltabr.pdf

Here I am going to go over a few basics of using `xltabr` to produce publication ready output. I'm using an extract of some already published Search and Rescue Helicopter data that was itself produced using `xltabr`, see the full series here. 

https://www.gov.uk/government/statistical-data-sets/search-and-rescue-helicopter-sarh01

#### Formatting prep

I'm assuming that the data has already been cleaned, tidied and the unformatted table created in R, most likely using `reshape2::dcast` (see chapter 4) as `xltabr` has bee designed to work well with `reshape2`. 

Read in table to be formatted

```{r}
demo_sarh <- readr::read_csv(
  file = "data/sarh_example.csv"
)
```

The formatting uses a style sheet for reference. This must be created in a .xlsx workbook, and the specific formatting applied to each cell. The cell text will be the name used to reference that style in the code. In the example below anything with style *header* applied to it will be bold, left-justified, and use the same font style, colour and size as in cell A1. It will also apply the row height, but this can be overwritten. You can also apply cell borders, as in styles *top_header_1*, *top_header_2* and *body_bold_method_change*.

![](data/sarh_dft_style_example.png)


```{r}
#Variables required to be manually updated  ------------------------------

#table_name_date variables - update these variables as required
latest_month <- "March 2018"
latest_year_end <- "Year ending 31 March 2018" 

#footer_text_admin footnote dates and information used - update these variables as required
last_updated <- "December 2017"
next_update <- "June 2018"
contact_telephone <- "some telephone number"
contact_email <- "some email address"

#hyperlink addresses used
hyperlink_sarh <- "https://www.gov.uk/government/collections/search-and-rescue-helicopter-statistics"
names(hyperlink_sarh) <- "Search and Rescue Helicopter Statistics"
class(hyperlink_sarh) <- "hyperlink"
#=========================================================================

# Titles ------------------------------------------------------------------
#standard dates used in table_name
table_name_date_01 <- paste0(": April 2017 to ", latest_month)
  
#function to minimise reptition in input for table titles
set_text_title <- function (table_number, table_name_subheader, table_name_date){
  title_text <- c("DEMO: Department for Transport Statistics"
                  ,"DEMO: Search and Rescue Helicopter Statistics"
                  , table_number
                  , paste0("DEMO: UK Civilian Search and Rescue Helicopter Taskings"
                          ,table_name_subheader
                          ,table_name_date)
                  ,"Number of Taskings")
  
  #check input is char and length 1, and output is length 5
  stopifnot(is.character(table_number)
            ,is.character(table_name_subheader)
            ,length(table_number) == 1
            ,length(table_name_subheader) == 1
            ,length(title_text) == 5)
  
  return(title_text)
}

#preset each style for each of the 5 rows of the title
set_style_title <- c("header"
                     ,"body"
                     ,"table_title"
                     ,"title"
                     ,"title_units")




# Footnotes ---------------------------------------------------------------
#standard footnotes used
#all tables have footer_text_source and footer_text_update footnotes,
#insert and number additional footnotes here/alter exisiting ones
footer_text_source <- "Source: demo data only"
footer_text_01 <- ": no data for some reason or other"
footer_text_02 <- c("methodology change")
footer_text_admin <- c(""
                       , paste0("Last updated: ", last_updated)
                       , paste0("Next update: ", next_update)
                       , paste0("Telephone: ", contact_telephone)
                       , paste0("Email: ", contact_email)
                       , ""
                       , "The figures in this table are fabricated for demonstration purposes."
)

#function to piece together individual footnotes for each table, based on those set above
set_text_footer <- function(...) {
  #check arguments are all char
  footer_text <- c(footer_text_source, ..., footer_text_admin)
}

#function to preset styles for footnotes (number of footnotes varies per table)
#all footnotes are in style "footer" except the first which is style "source"
#and the second to last 2 which are style "footer_italics"

set_style_footer <- function(footer_text){
  c("source"
    ,rep("footer", length(footer_text)-5) 
    ,"footer_italics"
    ,"footer_italics"
    ,"footer"
    ,"footer") 
}

#function to calculate the sheet row holding certain footer text, the search_text
#to be used in overiding row heights, (eg for wrapped text that goes over one line) and configuring hyperlinks
#if you give this function a vector it returns a vector of row positions
footer_text_row <- function(search_text){
  footer_text_row = (
    length(tab$title$title_text) #number of rows in title
    + length(tab$top_headers$top_headers_list) #number of rows in top header (usually 1)
    + dim(tab$body$body_df)[1]  #number of rows in body (not including header)
    + match(search_text, tab$footer$footer_text) #number of rows to search_text in footer
  ) 
  return(footer_text_row) #may be a single row number or a vector of row numbers
}


# Hyperlinks --------------------------------------------------------------
#functions to set the hyperlinks, function works by searching for the hyperlink text
#function uses variables that are already in the global environment
set_title_hyperlink <- function(){
writeData(wb = tab$wb
          , sheet = 1
          , startRow = match(x = names(hyperlink_sarh), table = tab$title$title_text) #find row of hyperlink by matching its 'name' the text that shows
          , startCol = 1
          , x = hyperlink_sarh)
}

# Table column widths -----------------------------------------------------
#assign to set consistent column width across all tables
table_column_width <- 15




```

#### Applying formatting
```{r}
# Formatting --------------------------------------------------------------
#when setting the formats for each table refer to script 02_sarh_format_prep
#for variables to use which are common to all tables

#set styles path
#xltabr::set_style_path("style/DfT_styles_SARH.xlsx")
xltabr::set_style_path("data/sarh_dft_style.xlsx")
#not all these row heights are making it through, eg title, body, footer stay as 12, 15, etc 
#top_headers changed to 26, 39... 
#some must be being overwritten by default (well hidden)

#openxlsx::openXL("data/sarh_dft_style.xlsx") #uncomment to check styles
#NB only edit the style sheet in Excel (editing/saving in LibreOffice does not work; it saves stuff slightly differently)

 
# demo_sarh ----------------------------------------------------------------
#Search and Rescue Helicopter Taskings - by Base

#initialise
tab <- xltabr::initialise()
#titles
tab <- xltabr::add_title(
  tab = tab
  , title_text = set_text_title(table_number = "DEMO-SARH"
                               , table_name_subheader = " - by Base"
                               , table_name_date = table_name_date_01)
  , title_style_names = set_style_title)
#top headers
top_header_row_1 <- c(rep("", dim(demo_sarh)[2] - dim(dplyr::select_if(demo_sarh , is.numeric))[2] + 1) #add 1 back to account for Year which is type int, but we want to skip it
                      , colnames(dplyr::select(demo_sarh, 3:dim(demo_sarh)[2]))
)
tab <- xltabr::add_top_headers(tab = tab
                              , top_headers = top_header_row_1) #add_top_headers uses top_header1 by default
#body table
tab <- xltabr::add_body(
  tab = tab
  , df = demo_sarh
  , row_style_names = c(rep("body_right", 3), "body_bold_right") #auto repeats the pattern
                        , fill_non_values_with = list(na = ':'))
#footnotes
#set footer text
footer_text <- set_text_footer(footer_text_01, footer_text_02)
tab <- xltabr::add_footer(
  tab = tab
  , footer_text = footer_text
  , footer_style_names = set_style_footer(footer_text))

#auto merge title and footer cells to width of table
tab <- xltabr::auto_merge_title_cells(tab)
tab <- xltabr::auto_merge_footer_cells(tab)

#set column widths
tab <- xltabr::set_wb_widths(tab, body_header_col_widths = table_column_width)

#set row heights
#row height for footnotes - do not hardcode row
openxlsx::setRowHeights(
  tab$wb
  , sheet = 1
  , rows = footer_text_row(search_text = footer_text_02)[1] #footer_text_02 has 2 rows, only want the first
  , heights = 2*12.75)

#set data and styles 
tab <- xltabr::write_data_and_styles_to_wb(tab)

#add further non-xltabr formatting
#enable hyperlinks in header and footer 
set_title_hyperlink()

#rename worksheet
openxlsx::renameWorksheet(wb = tab$wb, sheet = "Sheet1", newName = "DEMO-SARH")

#View and save output
#view output
openxlsx::openXL(tab$wb) 
#save output
openxlsx::saveWorkbook(tab$wb, file = "data/sarh_demo.xlsx", overwrite = TRUE)



```

  
 
### .sav

- haven to open SPSS, STAT, SAS files

## Connecting to databases

### SQL
 - DBI
 - default schema
  - link to C&C talks on the subject

### GCP
 - BigQuery - bigrquery C&C talk

<!--chapter:end:02-data-import.Rmd-->

# Table/Data Frame manipulation {#tables}

```{r include=FALSE}
library(dplyr)
library(readr)
library(reshape2)
library(tidyr)

# Read in required data using public data.gov extract
road_accidents <- readr::read_rds("data/road_accidents_2017.RDS")

```


This chapter provides an overview of code examples for table or data frame manipulation (a tidyverse data frame is referred to as a tibble).

One of the main things you will have to do in any R project or RAP project will be manipulating the data that you are using in order to get it into the format you require.

One of the main packages used to manipulate data is the dplyr package which we recommend and use throughout this book. The dplyr package (and others e.g. tidyr) are all part of the tidyverse. The tidyverse is a group of packages developed by Hadley Wickham and others and are all designed to work with each other. See https://www.tidyverse.org/ for more info.

**Tidyverse packages and functions can be combined and layered using the pipe operator `%>%`.**

Dplyr is built to work with **tidy data**. To find out more about tidy data please look at the following link https://r4ds.had.co.nz/tidy-data.html but the general principles are:

1. Each variables must have its own column
2. Each observation must have its own row
3. Each value must have its own cell



## Pivot and reshape tables

There will be two examples for pivoting tables provided:

* The tidyr package uses the gather/spread functions and is often used to create tidy data
* The reshape2 package is also a useful package to pivot tables and has added functionality such as providing totals of columns etc.


We want to have the day of the week variable running along the top so each day of the week is its own column.

```{r, echo = FALSE, results='asis'}
# Create smaller dataset for example
road_accidents_small <- road_accidents %>%
  dplyr::group_by(Accident_Severity, Day_of_Week) %>%
  dplyr::tally()

knitr::kable(head(road_accidents_small),
caption = "Number of road accidents by accident severity and weekday")

```



**Tidyr package**

Using the tidyr package, gather and spread functions can be used  to pivot the table views:

- **gather** makes wide data longer i.e. variables running along the top can be "gathered" into rows running down.

- **spread** makes long data wider i.e. one variable can be spread and run along the top with each value being a variable.

```{r}
# Pivot table using tidyr package
library(tidyr)
road_accidents_weekdays <- road_accidents_small %>%
  tidyr::spread(Day_of_Week, n)
  
```

With the spread function above you need to first specify the variable you want to spread, in this case Day_of_Week, and then the variable that will be used to populate the columns (n).

```{r, echo = FALSE, results='asis'}

knitr::kable(head(road_accidents_weekdays),
caption = "Number of road accidents by accident severity and weekday, tidyr::spread")

```


The opposite can also be done using the gather function: GET HELP ON THIS

```{r}
# Pivot table using tidyr package
library(tidyr)
road_accidents_gather <- road_accidents_weekdays %>%
  tidyr::gather(Accident_Severity, n, 1:7)
  
```


**Reshape2 package**

Again, this has two functions which can be used to pivot tables:

- **melt** makes wide data longer 

- **dcast** makes long data wider


```{r}
# Pivot table using reshape2 package
library(reshape2)
road_accidents_weekdays2 <- reshape2::dcast(road_accidents_small, Accident_Severity ~ Day_of_Week,
                                            value.var = "n")
  
```

With the dcast function above, after stating the name of the data frame, you need to specify the variable(s) you want in long format (multiple variables seperated by "+"), in this case Accident_Severity, and then the wide format variable(s) are put after the tilda (again multiple seperated by "+"). The value.var argument specifies which column will be used to populate the new columns, in this case it is n.

```{r, echo = FALSE, results='asis'}

knitr::kable(head(road_accidents_weekdays2),
caption = "Number of road accidents by accident severity and weekday, reshape2::dcast")

```

If you want to create sums and totals of the tables this can also be done using reshape2. For example, taking the original table, we want to pivot it and sum each severity to get the total number of accidents per day.

```{r}
# Pivot table using reshape2 package
library(reshape2)
road_accidents_weekdays3 <- reshape2::dcast(road_accidents_small, Accident_Severity ~ Day_of_Week,
                                            value.var = "n", sum, margins = "Accident_Severity")
  
```

In this example, we use the margins argument to specify what we want to combine to create totals. So we want to add all the accident severity figures up for each weekday. Before using margin you need to specify how the margins are calculated, in this case we want a sum. Alternative options are to calculate the length, i.e. the number of rows.

```{r, echo = FALSE, results='asis'}

knitr::kable(head(road_accidents_weekdays3),
caption = "Number of road accidents by accident severity and weekday plus totals, reshape2::dcast")

```




The opposite can also be done using the melt function.

```{r}
# Pivot table using reshape2 package
library(reshape2)
road_accidents_melt <- reshape2::melt(road_accidents_weekdays2, id.vars = "Accident_Severity",
                                      measure.vars = c("1", "2", "3", "4", "5", "6", "7"),
                                      variable.name = "Day_of_Week", value.name = "n")
  
```

For the melt function you need to specify:


id.vars = "variables to be kept as columns"

measure.vars = c("variables to be created as one column")

variable.name = "name of created column using the measure.vars"

value.name = "name of value column"


```{r, echo = FALSE, results='asis'}
knitr::kable(head(road_accidents_melt),
caption = "Number of road accidents by accident severity and weekday, reshape2::melt")
```


## Dropping and selecting columns

Use the dplyr select function to both select and drop columns.

**Select columns**

```{r}
road_accidents_4_cols <- road_accidents %>%
  dplyr::select(acc_index, Accident_Severity, Date, Police_Force)
```

```{r, echo = FALSE, results='asis'}
knitr::kable(head(road_accidents_4_cols),
caption = "Four columns from road accidents 2017")
```


**Drop columns**

 Note that to drop columns the difference is putting a "-" in front of the variable name.
 
```{r}
road_accidents_3_cols <- road_accidents_4_cols %>%
  dplyr::select(-Police_Force)
```


```{r, echo = FALSE, results='asis'}
knitr::kable(head(road_accidents_3_cols),
caption = "Three columns from road accidents 2017")
```
 


## Filtering data

Use the dplyr filter function to filter data.

This example filters the data for slight severity accidents (accident severity 3).

```{r}
road_accidents_slight <- road_accidents_4_cols %>%
  dplyr::filter(Accident_Severity == 3)
```


```{r, echo = FALSE, results='asis'}
knitr::kable(head(road_accidents_slight),
caption = "Slight severity road accidents 2017")
```

To filter multiple conditions:

And operator
```{r}
road_accidents_filter <- road_accidents_4_cols %>%
  dplyr::filter(Accident_Severity == 3 & Police_Force == 4)
```

Or operator
```{r}
road_accidents_filter2 <- road_accidents_4_cols %>%
  dplyr::filter(Accident_Severity == 3 | Accident_Severity == 2)
```

**Note: filtering with characters must be wrapped in "quotations" e.g:**
```{r, eval = FALSE}
road_accidents_filter3 <- road_accidents %>%
dplyr::filter(`Local_Authority_(Highway)` == "E09000010")

```
Also note that in the above example the variable is quoted in back ticks (`). This is because some variable names confuse R due to brackets and numbers and need to be wrapped in back ticks so R knows that everything inside the back ticks is a variable name.


## Get counts of data

To get counts for groups of data, the dplyr tally function can be used in conjunction with the dplyr group by function. This groups the data into the required groups and then tallys how many records are in each group.

```{r}
# Create grouped data set with counts
road_accidents_small <- road_accidents %>%
  dplyr::group_by(Accident_Severity, Day_of_Week) %>%
  dplyr::tally()
```

The above example creates groups by accident severity and weekday and counts how many accidents are in each group (one row equals one accident therefore the tally is counting accidents).

```{r, echo = FALSE, results='asis'}
knitr::kable(head(road_accidents_small),
caption = "Road accidents 2017 by accident severity and weekday")
```



## Combine tables

When combining data from two tables there are two ways to do this in R:

* Bind the tables by basically either appending the tables on the rows or columns
* Join the tables using the dplyr version of SQL joins

**Binding tables**

Binding tables is mainly done to append tables by creating more rows, however tables can also be binded by adding more columns. Although it is recommended to use the dplyr join functions to combine columns (see 5.6).

```{r include=FALSE}
library(dplyr)

# create three tables for example
accidents_1 <- dplyr::filter(road_accidents_small, Accident_Severity == 1)
accidents_2 <- dplyr::filter(road_accidents_small, Accident_Severity == 2)
accidents_3 <- dplyr::filter(road_accidents_small, Accident_Severity == 3)

```

Here are three tables, one shows data for accident severity of 1, one for accident severity of 2, and one for accident severity of 3.

```{r, echo = FALSE, results='asis'}

knitr::kable(accidents_1, caption = "Number of fatal road accidents in 2017, by weekday")
knitr::kable(accidents_2, caption = "Number of serious injury road accidents in 2017, by weekday")
knitr::kable(accidents_3, caption = "Number of slight injury road accidents in 2017, by weekday")

```

To combine these tables we can use the bind_rows function from the dplyr package. Use bind_rows when you want to append the tables underneath one another to make one longer table, i.e. you want to add more rows.

**Ensure that the column names for each table are exactly the same in each table.**

```{r}
# combine tables using bind_rows
library(dplyr)

all_accidents <- accidents_1 %>%
  dplyr::bind_rows(accidents_2, accidents_3)
  
```


```{r, echo = FALSE, results='asis'}

knitr::kable(all_accidents, caption = "Road accident data 2017, bind_rows")

```

```{r include=FALSE}
library(dplyr)

# create two tables for example
road_acc_1 <- head(dplyr::select(road_accidents, acc_index, Police_Force, Accident_Severity))
road_acc_2 <- head(dplyr::select(road_accidents, acc_index, Date, Day_of_Week))

```


## Joining tables


Joins in R can be done using dplyr. This is generally to combine columns of data from two tables:


```{r}
# combine tables using left join
library(dplyr)

all_accidents_cols_join <- road_acc_1 %>%
  dplyr::left_join(road_acc_2, by = "acc_index")
```

This uses the same principles as SQL, by specifying what the tables should be joined on using the **by =** argument. 


Dplyr has all the usual SQL joins for example, inner_join, full_join, right_join. All of these are used in the same way as the left join example above.

Another useful join for data manipulation is an anti_join. This provides all the data that is not in the joined table. For example, the below snapshot of a table displays road accident totals broken down by accident severity and weekday:

```{r, echo = FALSE, results='asis'}

knitr::kable(head(road_accidents_small, caption = "Road accident data 2017 by accident severity and weekday"))

```

I am interested in creating two sub-groups of this data, a table for all accidents on a Monday (weekday 2), and all other accidents.

First, I get the Monday data using the dplyr filter function (see 5.3).

```{r include=FALSE}
library(dplyr)

# create filtered Monday table for example

accidents_monday <- dplyr::filter(road_accidents_small, Day_of_Week == 2)

```

```{r, echo = FALSE, results='asis'}

knitr::kable(head(accidents_monday, caption = "Road accident data 2017 on a Monday by accident severity"))

```

Then, I can use an anti-join to create a table which has all of the data that is not in the above table:

```{r}
# create table of all rows not in the joined table
library(dplyr)

all_accidents_not_monday <- road_accidents_small %>%
  dplyr::anti_join(accidents_monday, by = c("Accident_Severity", "Day_of_Week"))
```

The above code takes the initial table we want to get our data from (road_accidents_small) and anti joins accidents_monday. This says, "get all the rows from road_accidents_small that are not in accidents_monday". Again, note the need to specify what the join rows would be joined and compared by.


```{r, echo = FALSE, results='asis'}

knitr::kable(all_accidents_not_monday, caption = "Road accident data 2017 not on a Monday by accident severity")
```

## Select specific columns in a join

Doing a join with dplyr will join all columns from both tables, however sometimes not all columns from each table are needed.

Let's look at some previous tables again:

```{r, echo = FALSE, results='asis'}

knitr::kable(road_acc_1, caption = "Police force and accident severity information for accidents")
knitr::kable(road_acc_2, caption = "Date and weekday information for accidents")
```

Let's say we want acc_index and Police_Force from the first table, and Date from the second table.

```{r}
# select specific columns from each table and left join
library(dplyr)

road_acc_3 <- road_acc_1 %>%
  dplyr::select(acc_index, Police_Force) %>%
  dplyr::left_join(select(road_acc_2, acc_index, Date), by = "acc_index")
```

The above code takes the first table and uses the select statement to select the required columns from the first table. 

Then within the left_join command, to select the data from the second table, you again add the select statement.

**Note: you will need to select the joining variable in both tables but this will only appear once**

```{r, echo = FALSE, results='asis'}

knitr::kable(road_acc_3, caption = "Police force and Date information for specific accidents")
```

## Sum rows or columns
These solutions use the base R functions rather than dplyr.

### Sum rows

To sum across a row:

```{r}
# sum across a row 
road_accidents_weekdays$rowsum <- rowSums(road_accidents_weekdays, na.rm = TRUE) 
```

```{r, echo = FALSE, results='asis'}

knitr::kable(road_accidents_weekdays, caption = "Road accidents 2017 by accident severity and weekday")
```

To sum across specific rows:

```{r}
# sum across specific rows 
road_accidents_weekdays$alldays <- road_accidents_weekdays$`1` + road_accidents_weekdays$`2`+
                                    road_accidents_weekdays$`3`+ road_accidents_weekdays$`4`+
                                    road_accidents_weekdays$`5`+ road_accidents_weekdays$`6`+
                                    road_accidents_weekdays$`7`
```

```{r, echo = FALSE, results='asis'}

knitr::kable(road_accidents_weekdays[,-9], caption = "Road accidents 2017 by accident severity and weekday")
```

### Sum columns
 
To sum columns to get totals of each column, ***note this will appear as a console output not in a data object***:

```{r, eval = FALSE}
# sum columns
colSums(road_accidents_weekdays, na.rm = TRUE) 
```


To get the totals of each column as a row in the data:

```{r}
library (janitor)
# create total column
road_accidents_weekdays <- road_accidents_weekdays %>%
  janitor::adorn_totals("row")
```

```{r, echo = FALSE, results='asis'}

knitr::kable(road_accidents_weekdays[,-9:-10], caption = "Road accidents 2017 by accident severity and weekday")
```


Reshape2 can also be used to get column totals when pivoting a table (See 5.1).

## Replace NAs or other values

```{r, include = FALSE}
# Create dataset for example with nas (need to change -1 value to na as this is how NAs are represented in the road accident open data)

# create nas
road_accidents_na <- road_accidents %>%
  dplyr::na_if(-1)

# get smaller data set for example
road_accidents_na <- road_accidents_na %>%
  head(n = 7) %>%
  dplyr::select(acc_index, `1st_Road_Class`, `2nd_Road_Class`, Junction_Control)

```

To replace all NAs in one column (Junction Control column) with a specific value:

```{r}
library (tidyr)
# replace all NAs with value -1
road_accidents_na$Junction_Control <- road_accidents_na$Junction_Control %>%
  tidyr::replace_na(-1)

```

**Note: To replace NA with a character the character replacement must be wrapped in "quotations"**

To replace all NAs in a data frame or tibble:

```{r}

# replace all NAs with value -1
road_accidents_na <- road_accidents_na %>%
  replace(is.na(.), -1)
  

```

To replace values with NA, specify what value you want to be replaced with NA using the na_if function:

```{r}

# create nas
road_accidents_na <- road_accidents_na %>%
  dplyr::na_if(-1)
  

```
**Note: to only create NAs in a specific column specify the column name in a similar manner to the first example in this section.**

To replace values:
```{r}

# replace 1st_road_class 
road_accidents_na <- road_accidents_na %>%
  dplyr::mutate(`1st_Road_Class` = dplyr::case_when(`1st_Road_Class` == 3 ~ "A Road",
                                      TRUE ~ as.character(`1st_Road_Class`)))
  

```

The case_when function is similar to using CASE WHEN in SQL. 

The TRUE argument indicates that if the values aren't included in the case_when then they should be whatever is after the tilda (~)  i.e. the equivalent of the ELSE statement in SQL.

The "as.character" parameter says that everything that isn't 3 should be kept as it is, this could be replaced by an arbitrary character or value e.g. "Other". This would make everything that is not a 3, coded as "Other". 

You can have multiple case_when arguments for multiple values, they just need to be seperated with a comma. Multiple case_when statements for different variables can be layered using the pipe operator `%>%`.



## Creating new variables

## Summarising data

## Look up tables

<!--chapter:end:03-table-manipulation.Rmd-->

```{r include=FALSE}
knitr::opts_chunk$set(collapse=TRUE, comment="#>")
```

# Working with dates {#dates}
**NOT FINISHED - need to write about `lubridate`**

<!--chapter:end:04-dates.Rmd-->

```{r include=FALSE}
knitr::opts_chunk$set(collapse=TRUE, comment="#>")
```

# Working with factors {#factors}

Working with factors can be tricky to both the new and the experience `R` user
as their behaviour is not always intuitive.  We can illustrate this with a 
couple of examples.  Firstly concatenating factors may not work as intended:

```{r}
my_factor <- factor(c("bob", "george", "bob"))
my_vector <- c(my_factor, my_factor)
my_vector
```

instead we would first need to convert to a character vector

```{r}
my_vector_2 <- c(as.character(my_factor), as.character(my_factor))
my_vector_2
```

Secondly, we may sometimes find a numeric vector is being stored as a factor:

```{r}
df <- read.csv("data/factor_example.csv")
my_vector <- df[[2]]
typeof(my_vector)
my_vector
```

to transform this to a numeric variable we can proceed as follows
```{r}
my_vector_2 <- as.numeric(levels(my_vector)[my_vector])
typeof(my_vector_2)
my_vector_2
```

To alleviate some of these issues it is useful to use the `forcats` package
**NOT FINISHED - need to write about `forcats`**

<!--chapter:end:05-factors.Rmd-->

# Plotting and Data Visualisations {#plots}

This chapter is for code examples of plotting and visualising data.


```{r include=FALSE}
# Every file must contain at least one R chunk due to the linting process.
```
Plotting in Base R

First read in data and summarise it so we're ready to plot:
```{r}

library(tidyverse)
library(readxl)

port <- read_excel("data/port0499.xlsx")

total_tonnage <- port %>%
  group_by(year) %>%
  summarise(total = sum(tonnage))

average_tonnage <- port %>%
  group_by(polu_majorport) %>%
  summarise(avg_tonnage = mean(tonnage)) %>%
  top_n(5, wt = avg_tonnage) %>%
  arrange(desc(avg_tonnage))

```
Line chart with Base R

```{r}
plot(total_tonnage$year, total_tonnage$total,
     type = "l",
     col = "red",
     lwd = 3,
     xlab = "Year",
     ylab = "Tonnage",
     main = "Total Tonnage 2000 - 2017")
```

Bar plot with Base R

```{r}
barplot(average_tonnage$avg_tonnage,
        width = 2,
        names.arg = c("Glensanda", "Sullom Voe", "Dover",
                      "Milford Haven", "Rivers Hull & Humber"),
        col = "lightblue",
        xlab = "Major Port",
        ylab = "Tonnage",
        main = "Average Tonnage by Major Port (top 5) 2000 - 2017")
```

Plotting with ggplot2

Line chart

```{r}
ggplot(data = total_tonnage) +
  geom_line(aes(x = year, y = total), col = "red", size = 1.5) +
  xlab("Year") +
  ylab("Tonnage") +
  ggtitle("Total Tonnage 2000 - 2017") +
  scale_x_continuous(breaks = seq(2000, 2017, 2)) +
  theme_classic()
```

Bar chart

<!--chapter:end:06-plotting.Rmd-->

# Interesting tidbits {#tidbits}

```{r include=FALSE}
# Every file must contain at least one R chunk due to the linting process.
```

## Rounding
For rounding numerical values we have the function `round(x, digits = 0)`.  
This rounds the value of the first argument to the specified number of decimal 
places (default 0).

```{r}
round(c(-1.5, -0.5, 0.5, 1.5, 2.5, 3.5, 4.5))
```

This probably looks different to what you were expecting as at school we were 
generally taught that when rounding a 5 we go up if a positive value and down if
a negative value.  R, however, implements a different standard.  We can see this
by looking at the documentation (`?round`)

> Note that for rounding off a 5, the IEC 60559 standard (see also IEEE 754) 
is expected to be used, *go to the even digit*. Therefore `round(0.5)` is `0` 
and `round(-1.5)` is `-2`. However, this is dependent on OS services and on 
representation error (since e.g. `0.15` is not represented exactly, the
rounding rule applies to the represented number and not to the printed number, 
and so `round(0.15, 1)` could be either `0.1` or `0.2`).

To implement what we consider normal rounding we can install use the `janitor`
package and the function `round_half_up`

```{r, warning=FALSE}
library(janitor)
janitor::round_half_up(c(-1.5, -0.5, 0.5, 1.5, 2.5, 3.5, 4.5))
```

If we do not have access to the package (or do not want to depend on the 
package) then we can implement^[see [stackoverflow](https://stackoverflow.com/questions/12688717/round-up-from-5/12688836#12688836)
discussion for a discussion of the implementation]

```{r}
round_half_up_v2 <- function(x, digits = 0) {
  posneg <- sign(x)
  z <- abs(x) * 10 ^ digits
  z <- z + 0.5
  z <- trunc(z)
  z <- z / 10 ^ digits
  z * posneg
}

round_half_up_v2(c(-1.5, -0.5, 0.5, 1.5, 2.5, 3.5, 4.5))
```

<!--chapter:end:08-tidbits.Rmd-->

